---
title: DRAFT - The Antarctic/Southern Ocean rOpenSci community
output:
  md_document:
    variant: markdown_github
---

```{r chunkopts, eval = TRUE, echo = FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, message = TRUE, warning = TRUE, tidy = FALSE, cache = FALSE, include = TRUE, dpi = 72, fig.width = 9, fig.height = 6, fig.align = "center", results = "markup")
```


### Antarctic/Southern Ocean science and rOpenSci

Collaboration and reproducibility are [fundamental to Antarctic and Southern Ocean science](https://doi.org/10.1038/d41586-018-05369-6), and the value of data to Antarctic science has long been promoted.
The [Antarctic Treaty](https://www.ats.aq/e/ats.htm) (which came into force in 1961) included the provision that scientific observations and results from Antarctica should be openly shared.
The high cost and difficulty of acquisition means that data tend to be re-used for different studies once collected. Further, there are many common data requirement themes (e.g. sea ice information is useful to a wide range of activities, from voyage planning through to ecosystem modelling).
Support for Antarctic data management is well established. The SCAR-COMNAP Joint Committee on Antarctic Data Management was established in 1997 and remains active (as a SCAR Standing Commitee)[https://www.scar.org/data-products/scadm/] today.

Software development to support Antarctic data usage is growing, but still lags the available data, and some common tasks are still more difficult than we would like.
Starting in late 2017, the [Scientific Committee on Antarctic Research](https://www.scar.org/) has been collaborating with rOpenSci to build the Antarctic and Southern Ocean R/science communities. Our focus is on data and tasks that are common or even unique to Antarctic and Southern Ocean science, including supporting the development of R packages to meet Antarctic science needs, guides for R users and developers, active fora for open discussions, and strengthening connections with the broader science world.


### First steps in building the community

- a couple of packages have been [formally](https://github.com/ropensci/antanym) [onboarded](https://github.com/ropensci/bowerbird)
- plethora of [in-development packages and supporting code](https://github.com/SCAR/antarctic-r-packages)
- some through formal SCAR groups, individual researchers or their institutions, other SO science/policy groups such as CCAMLR
- three common tasks:
  - getting hold of data. Environmental data are commonly needed, and often come from satellite, model, or similar sources (or large data collections)
  - processing those data to suit my study interests, often merging with field or other data
  - mapping

- demo below


### Get involved

Please get involved! Beyond the common rOpenSci channels ([Twitter feed](https://twitter.com/rOpenSci), [blog](https://ropensci.org/blog/), [discussion forum](https://discuss.ropensci.org/)):

- contribute an Antarctic R package, or improve the documentation or code of an existing one. See the draft [Antarctic R package list](https://github.com/SCAR/antarctic-r-packages) as a starting point

- contribute your Antarctic R knowledge, as a [code snippet](https://github.com/SCAR/rtools) or [tutorial](https://ropensci.org/tutorials/)

- join the developer #antarctic slack channel

- make [a suggestion](https://github.com/SCAR/ropensci/issues)


### Demo

First load some packages for general use.
```{r pkgs, message = FALSE, warning = FALSE}
## make sure we have the packages we need
req <- setdiff(c("dplyr", "ggplot2", "bsam", "remotes"), installed.packages())
if (length(req) > 0) install.packages(req)

## and some github packages
req <- c("AustralianAntarcticDivision/blueant", "AustralianAntarcticDivision/raadtools", "Maschette/SOmap", "ropensci/antanym")
req <- setdiff(basename(req), installed.packages())
if (length(req) > 0) remotes::install_github(req)
```

Let's say that we have some points of interest in the Southern Ocean --- perhaps a ship track, or some stations where we took marine samples, or as we'll use here, the [track of an elephant seal track](http://www.meop.net/) as it moves from the Kerguelen Islands to Antarctica and back again (Data from IMOS 2018, provided as part of the `bsam` package).

```{r get_track_data, message = FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)

data("ellie", package = "bsam")
x <- ellie %>% dplyr::filter(id == "ct96-05-13")
with(x, plot(lon, lat))
```

#### Fetching our environmental data

Very commonly, we want to know about the environmental conditions at our points of interest. For the remote and vast Southern Ocean these data typically come from satellite or models. Some data centres provide extraction tools that will pull out a subset of data to suit your requirements, but often it makes more sense to cache entire data collections locally first and then work with them from there:

- many analyses need a diverse range of data (in which case there may not be dynamic extraction tools for all of them)
- analyses might need to crunch through a whole collection of data in order to calculate appropriate statistics (temperature anomalies with respect to a long-term mean, for example)
- a common suite of data are routinely used by a local research community, in which case it makes more sense to keep a local copy for everyone to use, rather than multiple copies being downloaded by different individuals

In these cases, maintaining a local copy of a range of data from third-party providers can be extremely beneficial, especially if that collection is hosted with a fast connection to local compute resources (virtual machines or high-performance computational facilities).

[bowerbird](https://github.com/AustralianAntarcticDivision/bowerbird) provides a framework for downloading data files to a local collection, and keeping it up to date. The companion [blueant](https://github.com/AustralianAntarcticDivision/blueant) package provides a suite of definitions for Southern Ocean and Antarctic data sources that can be used with `bowerbird`.

Install the package if needed:
```{r eval = FALSE}
library(remotes)
install_github("AustralianAntarcticDivision/blueant")
```

To use `blueant`, we first choose a location to store our data. Normally this would be a persistent location (perhaps on shared storage if multiple users are to have access to it), but for the purposes of demonstration we'll just use a temporary directory here:

```{r echo = FALSE}
## in the background, use a persistent data dir so we don't endlessly re-download the data
my_data_dir <- "c:/temp/data"
```

```{r set_data_dir, eval = FALSE}
my_data_dir <- tempdir()
```

We'll focus on two sources of environmental data: sea ice and water depth. Note that water depth does not change with time but sea ice is highly dynamic, so we will want to know what the ice conditions are like on a day-to-day basis.

Download daily sea ice data (from 2013 only), and the ETOPO2 bathymetric data set. ETOPO2 is somewhat dated and low resolution compared to more recent data, but will do as a small dataset for demo purposes:

```{r get_env_data, eval = FALSE}
library(blueant)
src <- bind_rows(
    sources("NSIDC SMMR-SSM/I Nasateam sea ice concentration", hemisphere = "south", time_resolutions = "day", years = 2013),
    sources("ETOPO2 bathymetry"))
result <- bb_get(src, local_file_root = my_data_dir, clobber = 0, verbose = TRUE, confirm = NULL)
```


```{r really_get_env_data, echo = FALSE, message = FALSE, cache = TRUE}
## code not shown in output: capture the output and trim it down a bit
library(blueant)
src <- bind_rows(
    sources("NSIDC SMMR-SSM/I Nasateam sea ice concentration", hemisphere = "south", time_resolutions = "day", years = 2013),
    sources("ETOPO2 bathymetry"))
op <- capture.output(result <- bb_get(src, local_file_root = my_data_dir, clobber = 0, verbose = TRUE, confirm = NULL))
op[4] <- ""
op[5] <- " [... output truncated]"
for (oo in op[1:5]) cat(oo, "\n")
```

Now we have local copies of those data files. The sync can be run daily so that the local collection is always up to date - it will only download new files, or files that have changed since the last download.

Details of the files can be found in the `result` object, and those files could now be read with packages such as `raster`. However, we are collecting data from a range of sources, and so they will tend to be different in terms of their grids, resolutions, projections, and variable-naming conventions, which tends to complicate these operations. In the next section we'll look at the `raadtools` package, which provides a set of tools for doing common operations on these types of data.

#### Using those environmental data: raadtools

For Antarctic and Southern Ocean studies, it is very common to deal with satellite, model, or other environmental data. Such data are typically spatial or spatio-temporal in nature. `raadtools` is suite of functions that provide consistent access to a range of such data, and tools for working with them. It is designed to work with the `bowerbird`/`blueant` packages.

```{r message = FALSE, warning = FALSE}
library(raadtools)

## tell raadtools where our data collection has been stored
set_data_roots(my_data_dir)
```

Define our spatial region of interest and extract the bathymetry data from this region, using the files we just downloaded:

```{r}
roi <- round(c(range(x$lon), range(x$lat)) + c(-2, 2, -2, 2))
bx <- readtopo("etopo2", xylim = roi)
```

And now we can make a simple plot of our our track superimposed on the bathymetry:

```{r}
plot(bx)
lines(x$lon, x$lat)
```

We can also extract the depth values along our track using the `extract()` function in `raadtools`:

```{r}
x$depth <- extract(readtopo, x[, c("lon", "lat")], topo = "etopo2")
```

Plot the histogram of depth values, showing that most of the track points are located in relatively shallow waters:

```{r}
ggplot(x, aes(depth)) + geom_histogram(bins = 100) + theme_bw()
```

This type of extraction will also work with time-varying data --- for example, we can extract the sea-ice conditions along our track, based on each track point's location and time:

```{r}
x$ice <- extract(readice, x[, c("lon", "lat", "date")])
## points outside the ice grid will have missing ice values, so fill them with zeros
x$ice[is.na(x$ice)] <- 0
ggplot(x, aes(date, ice, colour = lat)) + geom_path() + theme_bw()
```

#### Mapping

Creating maps is another very common requirement, and in the Southern Ocean brings a few challenges (e.g. dealing with the dateline when using polar-stereographic or similar projections). There are also spatial features that many users want to show (coastlines, oceanic fronts, extent of sea ice, place names, etc).

```{r somap1}
library(SOmap)
default_somap(x$lon, x$lat)
```

Or a full-hemisphere map:
```{r somap2, fig.width = 8, fig.height = 8}
## first transform our track to polar-stereographic coordinates
library(sp)
library(raster)
xsp <- x
coordinates(xsp) <- c("lon", "lat")
projection(xsp) <- "+proj=longlat +ellps=WGS84"
xsp <- spTransform(xsp, CRS("+proj=stere +lat_0=-90 +lat_ts=-71 +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"))

## plot the base map
invisible(capture.output(SOmap()))
## add our track
plot(xsp, col = "darkgreen", add = TRUE)
```

#### Place names

SCAR maintains a gazetteer of place names in the Antarctic and surrounding Southern Ocean, which is available to R users via the `antanym` package:

```{r antanym1}
library(antanym)
xn <- an_read(cache = "session", sp = TRUE)
```

There is no single naming authority for place names in Antarctica, and so there can be multiple names for a single feature (where it has been given a name by more than one country). We can trim our names list down to one name per feature (here, preferentially choosing the UK name if there is one):

```{r antanym1b}
xn <- an_preferred(xn, origin = "United Kingdom")
```

There are a lot of named features in the Antarctic, and we can't show all of them on our map. Which ones should we show? Let's ask antanym for suggestions as to which names should be shown on this map

```{r antanym2, fig.width = 8, fig.height = 8}
xns <- an_suggest(xn, map_scale = 20e6, map_extent = c(-180, 180, -90, -40))

## transform to our map projection and take the first 10 names
temp <- as_tibble(spTransform(xns, projection(Bathy))) %>% head(10)

## add them to the map
invisible(capture.output(SOmap()))
plot(xsp, col = "darkgreen", add = TRUE)
with(temp, points(x = longitude, y= latitude, pch = 16))
with(temp, text(x = longitude, y= latitude, labels = place_name, pos = 2 + 2*(longitude > 0)))
```

### References

IMOS (2018) AATAMS Facility - Satellite Relay Tagging Program - Delayed mode data, https://catalogue-imos.aodn.org.au/geonetwork/srv/en/metadata.show?uuid=06b09398-d3d0-47dc-a54a-a745319fbece

Composite Gazetteer of Antarctica, Scientific Committee on Antarctic Research. GCMD Metadata (http://gcmd.nasa.gov/records/SCAR_Gazetteer.html)
